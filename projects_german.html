<!DOCTYPE HTML>
<!--
	Massively by HTML5 UP
	html5up.net | @ajlkn
	Free for personal and commercial use under the CCA 3.0 license (html5up.net/license)
-->
<html>
	<head>
		<title>Projekte / Dinh-An Ho</title>
		<meta charset="utf-8" />
		<meta name="viewport" content="width=device-width, initial-scale=1, user-scalable=no" />
		<link rel="stylesheet" href="assets/css/main.css" />
		<noscript><link rel="stylesheet" href="assets/css/noscript.css" /></noscript>
		<link rel="icon" type="image/png" href="images/icon.png">
	</head>
	<body class="is-preload">

		<!-- Wrapper -->
		<div id="wrapper">

			<!-- Header -->
				<header id="header">
					<a href="https://github.com/ikasu93?tab=repositories" class="logo">Projekte</a>
				</header>

			<!-- Nav -->
				<nav id="nav">
					<ul class="links">
						<li><a href="index_german.html">Über mich</a></li>
						<li class="active"><a href="projects_german.html">Meine Projekte</a></li>
						<li><a href="cv_german.html">Curriculum Vitae</a></li>
						<li><a href="gallery_german.html">Meine Kunst</a></li>
					</ul>
					<ul class="icons">
						<li><a href="https://twitter.com/ikasu13" class="icon brands fa-twitter"><span class="label">Twitter</span></a></li>
						<li><a href="https://www.linkedin.com/in/dinh-an-ho-82302b113/" class="icon brands fa-linkedin"><span class="label">Linkedin</span></a></li>
						<li><a href="https://github.com/ikasu93" class="icon brands fa-github"><span class="label">GitHub</span></a></li>
						<li><a href="mailto:hdinhan93@gmail.com" class="icon brands fa-google-plus-g"><span class="label"></span></a></li>
					</ul>
				</nav>

			<!-- Main -->
				<div id="main">
					<section class="posts">
								<article>
									<header>
										<h2>3D-Visualisierungs-API für selbstfahrende Autos (2017)</h2>
									</header>
									<a href="images/threejs.png" class="image fit"><img src="images/threejs.png" alt="" /></a>
									<p>In meiner Bachelorarbeit habe ich eine 3D-Visualisierung eines Auto-Modells und eine einfache Welt für Browser-Anwendungen entwickelt. Weiterhin wurde ein <a href= "http://www.monticore.de/languages/montiarc/">MontiArc</a> Controller für autonomes Fahren entworfen und eine API zwischen der Visualisierung, dem Controller und dem Server mit Java implementiert.</p>
									<p><div><details><summary><strong>Abstract</strong></summary>
<div align="justify" style="margin-top: 12px;">Aktuell sind selbstfahrende Autos ein großes Thema, vor allem weil
Qualität und Sicherheit wichtige Faktoren für die Menschheit sind. Die durch menschliche Fehler verursachte Sterblichkeit bei Verkehrsunfällen ist relativ hoch.
Forschungsthemen rund um das autonome Fahren und die damit verbundene Technik haben an Bedeutung gewonnen, wie
Wichtigkeit, aber auch in der Komplexität.
<br> </br>
Die 3D-Visualisierung des Forschungsfortschritts ist sinnvoll, damit die Software, die das autonome Auto steuert, simuliert und getestet werden, in grafischer Form getestet werden kann, bevor sie in einem realen Szenario verwendet wird. Dieses Projekt befasst sich mit der 3D-Visualisierung von selbstfahrenden Autos in Web Browsern. Die grafische Darstellung des Fahrzeugverhaltens hilft
den Mechanismus der Steuerung besser verstehen und bietet einen visuellen Überblick darüber. Neben der Visualisierung des Autos geht es auch um
die Frage, wie man ein Auto, das in einem Browser dargestellt wird, fernsteuert.<br><</br>
<p><b>Verfahren:</b>
<ul>
<li>Nach open-source .3ds-Modellen suchen und sie mit <a href="https://www.blender.org/">Blender</a> auf die erforderliche Weise verarbeiten</li> 
<li>Erstellen einer 3D-Umgebung mit <a href="https://threejs.org/" >Three.js</a></li>
<li>Implementierung der Simulation mit Java</li>
<li>Implementierung der Controller-Logik mit <a href="http://www.monticore.de/languages/montiarc/">MontiArc</a></li>
</ul>
<a href="images/logic.PNG" class="image fit"><img src="images/logic.PNG" alt=""" /></a>
</p>
</div>
</Details></div>
&nbsp;
</p>

<!--<p><b>Programming language:</b> Java, JavaScript, <a href="http://www.monticore.de/languages/montiarc/">MontiArc</a></p>-->
<ul class="actions special">
										<li><a href="https://github.com/ikasu93/Driving-Visualization" class="button">GitHub repository</a></li>
										<!--<li><a href="https://www.youtube.com/watch?v=tByHiV9v9Yk" class="button">Screencast</a></li>-->
									</ul>
<iframe width="500" height="300" src="https://www.youtube.com/embed/tByHiV9v9Yk" frameborder="0" allow="accelerometer; autoplay; clipboard-write; encrypted-media; gyroscope; picture-in-picture" allowfullscreen></iframe>
									
								</article>
								<article>
									<header>
										<h2>Ein Ethik-Guideline für Datenwissenschaftler (2020)</h2>
									</header>
									<a href="images/concept_overview2.png" class="image fit"><img src="images/concept_overview2.png" alt="" /></a>
									<p>Im Rahmen meiner Masterarbeit habe ich eine Guideline für Datenwissenschaftler entworfen, um Verzerrungen im Lebenszyklus der Datenwissenschaft zu erkennen und sich ihrer bewusst zu sein. Der Leitfaden ist als <a href="https://jupyter.org/">Jupyter Notebook</a> Erweiterung mit JavaScript und Python implementiert. Darüber hinaus wurden Funktionen integriert, die Tagging und Autocompletion ermöglichen.</p>
<p><div><details><summary><strong>Abstract</strong></summary>
<div align="justify" style="margin-top: 12px;">In den letzten Jahren ist die Datenwissenschaft zu einem unverzichtbaren Bestandteil unserer Gesellschaft geworden. Im Laufe der Zeit sind wir von dieser Technologie abhängig geworden, weil sie uns die Möglichkeit bietet, aus Daten in jedem Bereich - Wirtschaft, Sozialisation, Forschung und Gesellschaft - Wert und neue Erkenntnisse zu gewinnen. Die erste Phase der Datenwissenschaft konzentrierte sich auf die Genauigkeit und Effizienz von Daten und ihren Algorithmen. Die zweite Phase basiert nun auf der wachsenden Abhängigkeit - aufgrund der Konvergenz großer Datenmengen, des Cloud Computing, des Einsatzes neuer Technologien wie künstliche Intelligenz (KI), Algorithmen des maschinellen Lernens, die eine Automatisierung für einige Aufgaben mit besserer Leistung als der Mensch ermöglichen. Gleichzeitig wirft sie die Frage auf, inwieweit es gerechtfertigt ist, diesen Technologien zu vertrauen. Es besteht die Gefahr, dass solche Fähigkeiten zu voreingenommenen, unangemessenen oder unbeabsichtigten Handlungen führen können. Automatisierte Entscheidungen der KI sind unter Umständen nicht transparent und gegenüber bestimmten Gruppen unfair. Jeder Schritt im datenwissenschaftlichen Prozess (von den Rohdaten zu den Erkenntnissen
und Wissensabruf) können Verzerrungen hervorrufen. Daher sollten ethische Erwägungen, die sich aus den datenwissenschaftlichen Praktiken ergeben könnten, sorgfältig berücksichtigt werden. und diese potenziellen Probleme sollten während des Lebenszyklus der Datenwissenschaft identifiziert und wenn möglich gemildert werden.
<br></br>
Dies führt in den Bereich der verantwortlichen Datenwissenschaft (<a href="https://redasci.org/">RDS</a>).
Im Mittelpunkt der Aspekte von RDS steht die Datenethik, z.B. das Erreichen fairer, einvernehmlicher, transparenter Daten, die den datenwissenschaftlichen Algorithmen Klarheit verschaffen sollen. Es gibt einige Instrumente zur Datenethik, z.B. ethische Codes der ACM, des IEEE und der ASA, die sich mit Daten befassen. Diese Theorien müssen jedoch in praktische Ansätze eingebettet werden. Obwohl verschiedene Forscher ethische Bedenken in der KI geäußert haben, gab es nur sehr wenige Studien darüber, wie die datenwissenschaftliche Praxis diese Bedenken ausräumen kann. Ein typischer Datenwissenschaftler verfügt nicht über genügend Wissen, um diese Herausforderungen zu identifizieren, und es ist nicht immer möglich, einen Ethikexperten bei der Erstellung der Datenwissenschaft einzubeziehen. Deshalb in dieser Dissertation bieten wir einen praktischen Leitfaden für Datenwissenschaftler und erhöhen ihre Bewusstsein. Wir überprüften verschiedene Quellen von Verzerrungen und ordneten sie unter verschiedenen Phasen des datenwissenschaftlichen Lebenszyklus. 
<br></br>
Im Einzelnen haben wir die datenwissenschaftliche Pipeline und ihre Phasen analysiert. Es gibt verschiedene Modelle, aber wir haben jede Phase eines bestimmten Modells anhand von Benutzergeschichten beschrieben - von der Phase der Datenaufnahme bis zur Phase der Datenanalyse. Für jede Phase haben wir aufkommende Verzerrungen analysiert und sie mit Definitionen, Beispielen und bewährten Verfahren beschrieben. Mit diesem Wissen identifizierten wir dann ein Vokabular zur Definition von Konzepten. Danach erstellten wir eine unterstützende Ontologie. Dazu analysierten wir bestehende Ontologien und wählten spezifische Klassen aus, um
unsere eigene Ontologie zur Datenwissenschaft und Ethik erstellen. Mit unserer Ontologie ist es möglich, Wissen aus einem bestimmten Bereich zu extrahieren. Es ist möglich, Abfragen zu schreiben, die zum Beispiel bestimmte Methoden für einen bestimmten Bias extrahieren. Ausserdem ist es möglich, verschiedene Arten von Verzerrungen zu analysieren und zu erweitern. Als Erweiterung dazu wird es die Möglichkeit geben, aus bestimmten Schlüsselwörtern einen Wissensgraphen zu generieren, der z.B. für einen Bias angibt, in welcher Phase er abgerufen werden kann, welche Abschwächungsmethode angewendet werden kann, wie er sich zu anderen Bias verhält und ähnliches. Schließlich haben wir implementierte und analysierte ein Entwicklungstool zur Unterstützung von Daten, die Wissenschaftler kennen von bestehenden Verzerrungen im Lebenszyklus der Datenwissenschaft. Mit Hilfe des Leitinstruments ist es Es ist möglich, alle Phasen der Datenwissenschaft und alle sich abzeichnenden Verzerrungen zu untersuchen. Darüber hinaus Datenwissenschaftler können ihren Code mit einem Vokabular versehen. Schließlich ist das entwickelte Werkzeug von einem Informatiker auf seine Benutzerfreundlichkeit und User Experience hin bewertet.
<br></br>
<p><b>Prozedur:</b>
<ul>
<li>Implementierung <a href="https://github.com/ipython-contrib/jupyter_contrib_nbextensions">Jupyter-Notebook-Extension</a> mit JavaScript</li>
<li>Erstellung der Leitlinien-Visualisierung mit Python</li>
<li>Schaltflächen für Richtlinien mit Bias-Informationen verbinden als <i>readme.md</i> in einem GitHub-Repository</li>
<li>Autocompletion Funktion für Tagging und Keyword Eingabe mit Javascript einschließen</li></ul>
</p>
</div>
</details></div>
&nbsp;
</p>
<!--<a href="images/guideline.jpg" class="image fit"><img src="images/guideline.jpg" alt="" /></a>-->
<!--<p><b>Programming language:</b> jQuery, CSS, Python</p>-->
									<ul class="actions special">
										<li><a href="https://github.com/ikasu93/Data-Science-Bias-Guideline" class="button">GitHub Repository</a></li>
										<li><a href="https://arxiv.org/pdf/2009.09795.pdf" class="button">arXiv paper</a></li>
									</ul>
								</article>
								<article>
									<header>
										<h2>Explorative Analysis of a Biomedical Expert Survey to Validate Machine Learning based Patent Classification (2020)</h2>
									</header>
									<a href="images/ame.PNG" class="image fit"><img src="images/ame.PNG" alt="" /></a>
									<p>This project focuses on the data science and analysis of survey data by <a href="https://www.ame.rwth-aachen.de/go/id/nsnq/?lidx=1">AME</a> and is an important basis for an elaborated validation of their machine learning classifier of medical patents.
									</p>
									<p><div><details><summary><strong>Description</strong></summary>
<div align="justify" style="margin-top: 12px;">The <a href="https://www.ame.rwth-aachen.de/go/id/nsnq/?lidx=1">AME</a> conducted a survey with multiple biomedical experts to collect altogether over 1000 ratings on patents to validate the classifier. The goal is to determine the final clear label for a patent, which has multiple ratings of experts with potential differences in expertise for a specific field of the biomedical domain. 
The main challenge is to investigate the characteristics of the experts having impact on the ratings.
<br></br>
In this project, we have aggregated the answers of different patent survey participants to find a consistent classification. The average has proven that it provides unexpectedly accurate collective judgements. In this project, the broad approach would be to find weightings from a set of participants to aggregate all opinions into one. This is achieved by evaluating factors that are important for the final result. To find the factors, we have analyzed characteristics of the participants such as self-confidence, expertise or behavioral characteristics, vary them and observe the change. To find these characteristics in our project, I have analyzed the expert survey for different factors.
									<br></br>
									<p><b>Procedure:</b><ul>
										<li>I have analyzed the survey database with <a href="https://en.wikipedia.org/wiki/SQL_Server_Management_Studio"><i>SQL Server Management Studio</i></a>.</li>
										<li>I have saved important SQL outputs as CSV.</li>
										<li>The diagrams of data are plotted with <a href="https://matplotlib.org/"><i>matplotlib</i></a>. Additionally, I analyzed clusters of data with <i>Python</i>.</li>
										<li>Finally, I have developed an accumulation weighting model with <i>Python</i> and test it against a simple averaging model.</li>
									</ul>
									</p>
									</p></div></details></div>
									&nbsp;
									<!--<ul class="actions special">
										<li><a href="#" class="button">Check it out</a></li>
									</ul>-->
									<!--<p><b>Programming language:</b> SQL, Python</p>-->
								</article>
								<article>
									<header>
										<h2><a href="https://sites.google.com/site/softwarearchitectureinpractice/9-documenting-software-architecture/c-component-and-connector-c-c-views">C&ampC</a> Modeling and Middleware for Distributed Systems (2018)</h2>
									</header>
									<a href="images/component.PNG" class="image fit"><img src="images/component.PNG" alt="" /></a>
									<p></p>
									<p>	In a team of four students, we have built a bundle interconnected top level components of a model (let's assume we have a car model with several components which describes driving functions, etc.) into different clusters. The aim is to reduce connection and communication overhead between components by grouping affine components into different clusters which then are connected using a middleware (<a href= "https://www.ros.org/">ROS</a>). <p><div><details><summary><strong>Description</strong></summary>
									</p>
									<div align="justify" style="margin-top: 12px;">First of all, we have converted the <a href= "https://en.wikipedia.org/wiki/Symbol_table">symbol table</a> of a component into an adjacency matrix. Then, we have fed the adjacency matrix into the selected clustering algorithm. The clustering algorithm we used are the following: <a href="https://scikit-learn.org/stable/modules/generated/sklearn.cluster.SpectralClustering.html">Spectral clustering</a> and <a href="https://micans.org/mcl/">Markov clustering</a>. We generate middleware tags separating the clusters. This will build the cluster-to-ROS connections. A connection will be established if the target cluster label is different from the source cluster label thus connecting different clusters with each other. Finally, we have fed the result into existing manual clustering architecture. 
									Using <a href="https://en.wikipedia.org/wiki/Monte_Carlo_method">Monte Carlo simulation</a>, we then generated random costs for the cluster-to-ROS connectgions and compared these with the results of the costs from the adopted clustering algorithms.
									The following diagram shows the Monte Carlo costs for 1000 iterations and 3 cluster formations for the autopilot model.</p>
									<a href="images/montecarlo.png" class="image fit"><img src="images/montecarlo.png" alt="" /></a>
									As expected, the costs were significantly more expensive than those of spectral clustering (1813).
									<p><b>Programming language:</b> Java, <a href="http://www.monticore.de/languages/montiarc/">MontiArc (for modelling)</a></p>
									</div></details></div>
									&nbsp;
									
									<!--<ul class="actions special">
										<li><a href="#" class="button">Check it out</a></li>
									</ul>-->
								</article>
								<article>
										<header>
											<h2>Hackathon by e.GO Digital (2019)</h2>
										</header>
										<a href="images/ego.PNG" class="image fit"><img src="images/ego.PNG" alt="" /></a>
										<p>The target of the hackathon is the development of approaches for improving station-based carsharing. The given scenario is considered to be a landlord who offers e.Base Home, a carsharing service, to his tenants in a sub-urban neighbourhood. Although it was well received, it turned out that the service was not cost covering due to a low rate of utilization. Due to the landlord's desire to be climate-friendly, not only profitability is to be increased, but emissions are also to be reduced. Other factors to be considered are the improvement of charging behaviour of the users and the intuitive use of the interface. Moreover, an implementation of gamification is expected in order to enhance user engagement.</p>
										<div><details><summary><strong>Procedure</strong></summary>
<div align="justify" style="margin-top: 12px;"> We extended the existing application by two main features: repeated bookings and gamification. We developed our features using Python and for the user interface we additionally employed the framework Kivy. We set up a client-server architecture with "bottle" to simulate a realistic usage environment. The server can use the API to access the database (which includes booking and vehicle information). The other databases such as the ones containing user data and recurring booking information have been hacked together by storing python data types with pickles. The server repeatedly/regularly runs a task to check if the repeating bookings have been added to the booking database and if not, adds the missing bookings. Due to the lack of real data, we generated fictional bookings to prove the feasability of our implementation. The main.py file provides the graphical user interface functionality and uses the functions in the client.py file to communicate with the server.
	The server.py should be started as a background process first and will listen for requests from the client. It uses functions from the control.py and api_control.py files to access the database trough the API.</div>
</details></div>
&nbsp;
										<ul class="actions special">
											<li><a href="https://github.com/nqthong11/Banh-Mi" class="button">GitHub Repository</a></li>
										</ul>
								</article>
							</section>
				</div>
		<!-- Footer -->
			<footer id="footer">
			<center>
				<div>	
					<span></span>
					<br></br>
					<div>
					<h2 id="test">Nehmen wir Kontakt auf!</h2>
					</div>
					<br></br>
					<h2 id="fuss">
						<a href="https://www.linkedin.com/in/dinh-an-ho-82302b113/"><i class="fab fa-3x fa-linkedin fa-lg"></i></a>&nbsp;&nbsp;
						<a href="mailto:hdinhan93@gmail.com"><i class="fas fa-3x fa-envelope-square fa-lg"> </i></a>&nbsp;&nbsp;
						<a href="tel:+49 176 34746475"><i class="fas fa-3x fa-phone-square fa-lg"> </i></a>
					</h2>
					<br></br>
				</div>
			</center>
			</footer>
		<!-- Copyright -->
			<div id="copyright">
				<ul>
					<li><a href="projects.html"> English</a></li>
					<li>&copy; Dinh-An Ho Portfolio</li>
					<li>Design: <a href="https://html5up.net">HTML5 UP</a></li>
				</ul>
			</div>

	</div>

		<!-- Scripts -->
			<script src="assets/js/jquery.min.js"></script>
			<script src="assets/js/jquery.scrollex.min.js"></script>
			<script src="assets/js/jquery.scrolly.min.js"></script>
			<script src="assets/js/browser.min.js"></script>
			<script src="assets/js/breakpoints.min.js"></script>
			<script src="assets/js/util.js"></script>
			<script src="assets/js/main.js"></script>

	</body>
</html>